---
title: 'Análisis Estadístico con R'
subtitle: "Series de Tiempo"
author: 
- name: Víctor Morales-Oñate
  affiliation: "[RUsersEC](http://rusersgroup.com/)"
date: "21 de marzo de 2018"
output: 
  pdf_document:
    toc: true
    toc_depth: 2
bibliography: bibliography.bib
---
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({ TeX: { equationNumbers: {autoNumber: "all"} } });
  </script>
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = normalizePath("~/Documents/Consultorias&Cursos/DataLectures")) 
```

# Introducción

Una serie de tiempo es una sucesión de variables aleatorias ordenadas de acuerdo a una
unidad de tiempo, $Y_{1}, \ldots, Y_{T}$.


¿Por qué usar series de tiempo?

-  Pronósticos
-  Entender el mecanismo de generación de datos (no visible al inicio de una investigación)


# [Rezagos y operadores en diferencia](http://www.economia.unam.mx/biblioteca/redeco/Pdf/Rezagos%20y%20Operadores%20de%20Diferencia.pdf)

##  Operadores de rezagos

Definción:

$$
\Delta Y_{t-i} = Y_t-Y_{t-i}
$$

Ejemplos:

$$
\Delta Y_{t} = Y_t-Y_{t-1}
$$

Caso general:

$$
L^{j}Y_t=Y_{t-j}
$$

Ejemplos:

$$
L^1Y_t=LY_t=Y_{t-1}
$$
$$
L^2Y_t=Y_{t-2}
$$

$$
L^{-2}Y_t=Y_{t+2}
$$

$$
L^{i}L^{j}=L^{i+j}=Y_{t-(i+j)}
$$
# Manipulando `ts` en `R

-   Abrir `IPCEcuador.csv`
-   Se puede ver una inflación variable

```{r}
uu <- "https://raw.githubusercontent.com/vmoprojs/DataLectures/master/IPCEcuador.csv"
datos <- read.csv(url(uu),header=T,dec=".",sep=",")
IPC <- ts(datos$IPC,start=c(2005,1),freq=12)
plot(IPC)
```

La serie tiene tendencia creciente. Tratemos de quitar esa tendencia:

```{r}
plot(diff(IPC)) # Se puede ver una inlfacion estable
abline(h=0)
```


Se ha estabilizado, pero podemos hacerlo aún más con el logartimo de la diferencia:

```{r}
plot(diff(log(IPC))) #Tasa de variacion del IPC
```

La serie no tiene tendecia y es estable. Ahora, si deseo trabajar con un subconjunto de datos, puedo...

```{r}
# Solo quiero trabajar con los datos de agosto 2008
IPC2 <- window(IPC,start=c(2008,8),freq=1)
plot(IPC2)

# IPC de todos los diciembres
IPC.dic <- window(IPC,start=c(2005,12),freq=T)
plot(IPC.dic)
points(IPC.dic)
```

Si tengo mensuales y necsito trabajar con el IPC anual:

```{r}
aggregate(IPC)
```

A continuación algunas transformaciones frecuentes y su [interpretación](https://www.ucm.es/data/cont/docs/518-2013-10-25-Tema_6_EctrGrado.pdf):


| Transformación                | Interpretación                               |
|--------------------------------------|--------------------------------------------------|
| $z_t=\nabla y_t=y_t-y_{t-1}$ | Cambio en $y_t$. Es un indicador de crecimiento absoluto.                   |
| $z_t=ln(y_t)-ln(y_{t-1})\approx \frac{y_t-y_{t-1}}{y_{t-1}}$ | Es la tasa logarítmica de variación de una variable. Es un indicador de crecimiento relativo. Si se multiplica por 100 es la tasa de crecimiento porcentual de la variable                   |
|$z_t=\nabla[ln(y_t)-ln(y_{t-1})]$   | Es el cambio en la tasa logarítmica de variación de una variable. Es un indicador de la aceleración de la tasa de crecimiento relativo de una variable. |


### Ejemplo



Veamos un gráfico más interesante usando un conjunto de datos anterior, vamos a:

-   Abrir la base `estadísticas Turismo.csv`
-   Agregar de manera mensual
-   Convertir a `ts` y graficar

```{r,message=FALSE}
uu <- "https://raw.githubusercontent.com/vmoprojs/DataLectures/master/estadisticas%20Turismo.csv"
datos<-read.csv(url(uu),header=T,dec=".",sep=";")
attach(datos)

# Visitas a Areas Naturales Protegidas

# Sumar por mes y año

mensual<-aggregate(TOTALMENSUAL,by=list(mesnum,Year),FUN="sum") # Los datos sin mes es el total de ese anio

TOTALmensual<-ts(mensual[,3],start=c(2006,1),freq=12)
plot(TOTALmensual)
```

Se ve una tendencia creciente y también una cierta estacionalidad. Veamos la misma serie en gráficos más atractivos:

```{r,message=FALSE,warning=FALSE}
library(latticeExtra)
library(RColorBrewer)
library(lattice)
xyplot(TOTALmensual)
asTheEconomist(xyplot(TOTALmensual,
                      main="TOTAL VISITAS MENUSALES \n AREAS PROTEGIDAS")
               ,xlab="Year_mes",ylab="Visitantes")
```


# Descomposición de una serie de tiempo

Componentes

-   Tendencia-ciclo: representa los cambios de largo plazo en el nivel de la serie de tiempo
-   Estacionalidad:  Caracteriza fluctuaciones periódicas de longitud constante causadas por factores tales como temperatura, estación del año, periodo vacacional, políticas, etc.

$$
Y_t = f(S_t,T_t,E_t)
$$
donde $Y_t$ es la serie observada, $S_t$ es el componente estacional, $T_t$ es la tendencia y $E_t$ es el término de error.

La forma de $f$ en la ecuación anterior determina tipos de [descomposiciones](https://seriesdetiempo.files.wordpress.com/2012/08/descomposicic3b3n-de-series-de-tiempo-base-cap-3-makridakis-et-al-fe-ago-2012.pdf):

| Descomposición                | Expresión                               |
|--------------------------------------|--------------------------------------------------|
| Aditiva | $Y_t = S_t +T_t+E_t$                   |
| Multiplicativa | $Y_t = S_t *T_t*E_t$                   |
|  Transformación logaritmica| $log(Y_t) = log(S_t) +log(T_t)+log(E_t)$ |
|Ajuste estacional|$Y_t - S_t =T_t+E_t$|

### Ejemplo

```{r}
visitas.descompuesta<-decompose(TOTALmensual, type="additive")
plot(visitas.descompuesta)
```

Dentro de `visitas.decompuesta` tenemos los siguientes elementos:

-   `$x` = serie original
-   `$seasonal` = componente estacional de los datos EJ: en marzo hay un decremento de 2502 (para cada dato)
-   `$trend` = tendencia
-   `$random` = visitas no explicadas por la tendencia o la estacionalidad
-   `$figure` = estacionalidad (mismo que `seasonal` pero sin repetición)

### Descomposición: ¿aditiva o multiplicativa?

Visualmenete:

- Aditivo: 
    -   las fluctuaciones estacionales lucen aproximadamente constantes en tamaño con el tiempo y 
    -   no parecen depender del nivel de la serie temporal, 
    -   y las fluctuaciones aleatorias también parecen ser más o menos constantes en tamaño a lo largo del tiempo
    
- Multiplicativo
      -   Si el efecto estacional tiende a aumentar a medida que aumenta la tendencia
      -   la varianza de la serie original y la tendencia aumentan con el tiempo


Forma alternativa de elegir: ver cuál es la que tiene un componente aleatorio menor.

### Ejemplo:

Los datos en el archivo `wine.dat` son ventas mensuales de vino australiano por categoría, en miles de litros, desde enero de 1980 hasta julio de 1995. Las categorías son blanco fortificado (`fortw`), blanco seco (`dryw`), blanco dulce (`sweetw`), rojo (`red`), rosa (`rose`) y espumoso (`spark`).


```{r}
direccion<- "https://raw.githubusercontent.com/dallascard/Introductory_Time_Series_with_R_datasets/master/wine.dat"
wine<-read.csv(direccion,header=T,sep="")
attach(wine)
head(wine)
dulce <- ts(sweetw,start=c(1980,12), freq=12)
plot(dulce)
```


Tratemos la serie como un caso aditivo:

En funcion del grafico de la variable, se decide el "type" de la descomposicion
La estacionalidad tiene valores negativos porque se plantea respecto de la tendencia

```{r}
dulce.descompuesta<-decompose(dulce, type="additive") 
plot(dulce.descompuesta)

a<-dulce.descompuesta$trend[27] # La tendencia era de 130 en mayo del 82
b<-dulce.descompuesta$seasonal[27] # El componente de la estacionalidad era este
c<-dulce.descompuesta$random[27]  # Es el componente aleatorio

a+b+c # La sumatoria de la descomposicion de la serie da el valor real, si es aditiva
```


Veamos el caso multiplicativo:

```{r}
# Multiplicativa
dulce.descompuesta1<-decompose(dulce, type="multiplicative")
plot(dulce.descompuesta1)
```

```{r}
a<-dulce.descompuesta1$trend[27] # La tendencia era de 130 en mayo del 82
b<-dulce.descompuesta1$seasonal[27] # El componente de la estacionalidad era este
c<-dulce.descompuesta1$random[27]  # Es el componente aleatorio

a*b*c # La sumatoria de la descomposicion de la serie da el valor real, si es aditiva
```



Veamos la forma alternativa de elección:

```{r}
u1<-var(dulce.descompuesta$random,na.rm=T)
u2<-var(dulce.descompuesta1$random,na.rm=T)
cbind(u1,u2)
```

Se escoge la multiplicativa en este caso.

### Detrend:

Las series se ofrecen generalmente sin tendencia ni estacionalidad. Veamos la serie sin tendencia:

```{r}
dulce.detrended <- dulce-dulce.descompuesta$trend
plot(dulce.detrended)
abline(h=0)
```

Parece ser que hay un cambio en la varianza desde el 85.

Si descomponemos multiplicativamente en vez de restar se debe dividir.

```{r}
plot(dulce-dulce.descompuesta$trend)
plot(dulce/dulce.descompuesta1$trend)
```

Existen formas de descomponer más sofisticadas, por ejemplo, usando la función `stl`.

```{r}
dulce.stl<-stl(dulce,s.window="per")
plot(dulce.stl)
```

En este caso el calculo de la tendencia cambia, se calcula con formas no paramétricas. La barra del final es la desviacion estándar.

## Suavizamiento: Holt-Winters

El método se resume en las fórmulas siguientes:

\begin{eqnarray}
a_{t} &=& \alpha(x_{t}-s_{t-p})+(1-\alpha)(a_{t-1}+b_{t-1}) \nonumber \\
b_{t} &=& \beta(a_{t}-a_{t-1})+(1-\beta)b_{t-1} \nonumber \\
s_{t} &=& \gamma(x_{t}-a_{t})+(1-\gamma)s_{t-p} \nonumber
\end{eqnarray}

El método de Holt-Winters generaliza el método de suavizamiento exponencial.

### Ejemplo 

Veamos un modelo más sencillo:

\begin{eqnarray}
x_{t} &=& \mu_{t}+w_{t} \nonumber \\
\mu_{t}=a_{t}&=& \alpha x_{t} + (1-\alpha) a_{t-1} \nonumber
\end{eqnarray}

```{r}
dulce.se <- HoltWinters(dulce,beta=0,gamma=0)
plot(dulce.se) 
```

Es un suavizamiento HW sin tendencia y sin componente estacional.
La serie roja son los datos con suavizamiento exponencial y la negra son los observados. `R` buscó el alpha que le pareció apropiado.

Usemos un alpha deliberado:

```{r}
dulce.se1 <- HoltWinters(dulce,alpha=0.8,beta=0,gamma=0)
plot(dulce.se1)
```

¿Qué pasó con los errores?

```{r}
dulce.se$SSE # Suma de los residuos al cuadrado (de un paso)
dulce.se1$SSE # Suma de los residuos al cuadrado (de un paso)
```


Es decir, el criterio para la busqueda de los parámetros es la minimización del SSE.

### Ejemplo

Total mensual de pasajeros (en miles) de líneas aéreas internacionales, de 1949 a 1960.

```{r}
data(AirPassengers)
str(AirPassengers)
plot(AirPassengers)
```

Se aprecia tendencia y variabilidad. Podemos usar HW para predicción:

```{r}
ap.hw<- HoltWinters(AirPassengers,seasonal="mult")
plot(ap.hw)
ap.prediccion <- predict(ap.hw,n.ahead=48)
ts.plot(AirPassengers,ap.prediccion,lty=1:2,
col=c("blue","red"))
```


# Modelos de series de tiempo


## Ruido blanco

Una serie $(\epsilon_t,t\in \mathbb{Z)}$ se dice que es Ruido Blanco si cumple

-   $E(\epsilon_t)=0$ (media cero)
-   $Var(\epsilon_t)=\sigma^2$ (varianza constante)
-   $\forall k\neq 0$, $Cov(\epsilon_t,\epsilon_{t+k})=0$ (Incorrelación)


Si además cumple que $\epsilon_t\sim N(0,\sigma^2)$ se dice que $\epsilon_t$ es Ruido Blanco Gaussiano (RBG).


```{r}
n   <-200
mu  <- 0
sdt <- 3
w <- rnorm(n,mu,sdt)
```


¿Cómo se si algo tiene ruido blanco? : Analizo la función de autocorrelación muestral.

$$
\hat\rho_k = \frac{\hat\gamma_k}{\hat\gamma_0}
$$
donde
$$
\hat\gamma_k = \frac{\sum(Y_t-\bar{Y})(Y_{t-k}-\bar{Y})}{n}
$$
$$
\hat\gamma_0 = \frac{\sum(Y_t-\bar{Y})^2}{n}
$$


Se asume que $\hat\rho_k \sim N(0,1/n)$. Es decir:

$$
\hat\rho_k = \frac{\sum_{t=k+1}^{T}(Y_t-\bar{Y})(Y_{t-k}-\bar{Y})}{\sum_{t=1}^{T}(Y_t-\bar{Y})^2}
$$

```{r}
acf(w)
```

Si se sale de las franjas, si hay correlación y no hay ruido blanco


## Serie estacionaria (en covarianza)

Una serie $(Y_t, t \in Z)$ se dice estacionaria en covarianza o simplemente estacionaria si cumple dos condiciones:

1.   $E(Y_t)=\mu$
2.   $Cov(Y_{t_1},Y_{t_2})=R(t_2-t_1)$ con $R$ función par ($f(-x)=f(x)$)

Es decir, la covarianza entre $Y_{t_1}$ y $Y_{t_2}$ depende únicamente de la distancia entre los tiempo $t_2$ y $t_1$, $|t_2-t_1|$.

# Procesos ARMA(p,q)

En los modelos de descomposición $Y_t = T_t + S_t + \epsilon_t$, $t = 1, 2,\ldots$ se estima $\hat\epsilon_t$ y se determina si es o no ruido blanco mediante, por ejemplo, las pruebas LjungBox y DurbinWatson.

En caso de encontrar que $\hat\epsilon_t$ no es ruido blanco, el siguiente paso es modelar esta componente mediante tres posibles modelos:

1. Medias Móviles de orden $q$, $MA(q)$.
2. Autoregresivos de orden $q$, $AR(p)$.
3. Medias Móviles Autoregresivos, $ARMA(p, q)$.
 
# El modelo Autoregresivo AR(p)

Se dice que $Y_n$, $n \in \mathbb{Z}$ sigue un proceso $AR(p)$ de media cero si


$$
Y_t = \phi_1Y_{t-1}+\phi_2Y_{t-2}+\cdots+\phi_pY_{t-p}+\epsilon_t
$$

donde $\epsilon_t\sim RB(0,\sigma^2)$ y $p = 1,2,\ldots$. Usando el operador de rezago $L$ se puede escribir como:

$$
\phi_p(L)(Y_n) =\epsilon_n
$$

con $\phi_p(z)=1-\phi_1z-\phi_2z^2-\cdots-\phi_pz^p$,el polinomio autorregresivo.


**Condición Suficiente para que un $AR(p)$ sea Estacionario en Covarianza**

La condición suficiente para que $Y_t \sim AR(p)$ sea estacionario en covarianza es que las $p$ raíces de la ecuación $\phi_p(z) = 0$, $z_i$, $i =1,2,\ldots,p$ cumplan

$$
|z_i|>1.\label{eq2}
$$

En palabras, la condición $\ref{eq2}$ se describe como *para que un proceso autorregresivo de orden $p$ sea estacionario en covarianza, es suficiente que las raíces del polinomio autorregresivo estén por fuera del círculo unitario*

Si el proceso $Y_t$ es estacionario en covarianza se cumple que su media es constante, $Y_t = \mu$

**Propiedades**

1. $E(Y_t) = 0$
2. $\sum_{j=1}^{p}\phi_j<1$


<!-- Proceso estocástico autoregresivo de primer orden -->

<!-- \begin{eqnarray} -->
<!-- (Y_{t}-\delta) = \alpha_{1}(Y_{t-1}-\delta)+u_{t} \nonumber -->
<!-- \end{eqnarray} -->

<!-- Donde $\delta$ es la media de $Y$ y $u_{i}$ es un ruido blanco no correlacionado. -->

Trabajaremos con datos de $M1$ ([WCURRNS](https://fred.stlouisfed.org/series/WCURRNS) dinero en circuación fuera de los Estados Unidos) semanales de los Estados Unidos desde enero de 1975.

```{r}
uu <- "https://raw.githubusercontent.com/vmoprojs/DataLectures/master/WCURRNS.csv"
datos <- read.csv(url(uu),header=T,sep=";")
names(datos)
attach(datos)
value.ts <- ts(VALUE, start=c(1975,1),freq=54)
ts.plot(value.ts)
```

Estacionariedad: La serie es estacionaria si la varianza no cambia

```{r}
acf(value.ts)
```

Esta es la marca de una serie que NO es estacionaria, dado que la autocorrelación decrece muy lentamente.


```{r}
plot(stl(value.ts,s.window="per"))  
```

Una forma de trabajar con una serie esacionaria es quitarle el *trend*

```{r}
valuetrend<- value.ts- stl(value.ts,s.window="per")$time.series[,2]
plot(valuetrend)
```

*Reminder* es lo que queda sin tendencia ni estacionalidad

```{r}
valuereminder<-
stl(value.ts,s.window="per")$time.series[,3]
```

Veamos cómo quedo la serie:

```{r}
acf(valuereminder)
```

Se puede decir que la hicimos una serie estacionaria

Otra forma de hacer estacionaria una serie es trabajar con las diferencias

```{r}
acf(diff(value.ts))
```


Nos indica que hay una estructura en la serie que no es ruido blanco pero SI estacionaria (cae de 1 a "casi"" cero)

### Simulación:

El siguiente paso es modelar esta estructura. Un modelo para ello es un modelo autorregresivo. Simular un $AR(1)$.

```{r}
y <- arima.sim(list(ar=c(0.99),sd=1),n=200)
plot(y)
acf(y)
```

¿Cuáles son los parámetros del `arima.sim`? Hemos simulado $Y_t = \phi_{0}+\phi_{1}Y_{t-1} = \phi_{0}+0.99Y_{t-1}$.

Simulemos el modelo: $Y_t = 0.5Y_{t-1} - 0.7Y_{t-2} + 0.6Y_{t-3}$

```{r}
ar3 <- arima.sim(n=200,list(ar=c(0.5,-0.7,0.6)),sd=5) 
ar3.ts = ts(ar3)
plot(ar3.ts)
acf(ar3)
```

Las autocorrelaciones decaen exponencialemente a cero

**Autocorrelaciones parciales**: nos ayunda a determinar el orden del modelo.

La autocorrelación parcial es la correlación entre $Y_t$ y $Y_{t-k}$ después de eliminar el efecto de las $Y$ intermedias.


**Definición**
Suponga que $(Y_t, t \in Z)$ es estacionaria. La pacf muestral es una función de $k$,

1. $\hat{\alpha}(1)=\hat\rho(1)$
2. $\hat{\alpha}(2)$: se regresa $Y_t$ sobre $Y_{t-1}$ y $Y_{t-2}$ tal que $Y_t=\phi_{21}Y_{t-1}+\phi_{22}Y_{t-2}+\epsilon_t$ entonces $\hat{\alpha}(2)=\phi_{22}$
3. $\hat{\alpha}(k)$: se regresa $Y_t$ sobre $Y_{t-1}\ldots Y_{t-k}$ tal que $Y_t=\phi_{k1}Y_{t-1}+\ldots+\phi_{kk}Y_{t-2}+\epsilon_t$ entonces $\hat{\alpha}(k)=\phi_{kk}$

En los datos de series de tiempo, una gran proporción de la correlación entre $Y_t$ y $Y_{t-k}$ puede deberse a sus correlaciones con los rezagos intermedios $Y_1,Y_2,\ldots,Y_{t-k+1}$. La correlación parcial elimina la influencia de estas variables intermedias.


```{r}
pacf(ar3) 
ar(ar3)$aic
```

La tercera autocorrelación es la que esta fuera de las bandas, esto indica que el modelo es un AR(3)

#### Ejemplo

Datos: precio de huevos desde 1901

```{r}
uu <- "https://raw.githubusercontent.com/vmoprojs/DataLectures/master/PrecioHuevos.csv"
datos <- read.csv(url(uu),header=T,sep=";")
ts.precio <- ts(datos$precio,start=1901)
plot(ts.precio)
```

Veamos las autocorrelaciones:

```{r}
par(mfrow=c(2,1))
acf(ts.precio)
pacf(ts.precio)
par(mfrow=c(1,1))
```

Las auto si decaen, no lo hacen tan rápido. No se puede decir si es estacionario o no.

Evaluemos un modelo:

```{r}
modelo1 <- arima(ts.precio, order=c(1,0,0))
print(modelo1)
modelo1$var.coef
```

¿Qué nos recomienda `R`?

```{r}
ar.precio <- ar(ts.precio)
ar.precio
```




Analicemos los residuos



```{r}
residuos = ar.precio$resid
# Los residuos debe estar sin ninguna estructura
par(mfrow = c(2,1))
plot(residuos)
abline(h=0,col="red")
abline(v=1970,col="blue")
acf(residuos,na.action=na.pass)
```

### Prueba de Ljung-Box

La prueba de Ljung-Box se puede definir de la siguiente manera.

$H_0$: Los datos se distribuyen de forma independiente (es decir, las correlaciones en la población de la que se toma la muestra son 0, de modo que cualquier correlación observada en los datos es el resultado de la aleatoriedad del proceso de muestreo).


$H_a$: Los datos no se distribuyen de forma independiente.

La estadística de prueba es:

$$
Q=n\left(n+2\right)\sum _{k=1}^{h}{\frac {{\hat {\rho }}_{k}^{2}}{n-k}}
$$

donde n es el tamaño de la muestra, $\hat\rho_{k}$ es la autocorrelación de la muestra en el retraso k y h es el número de retardos que se están probando. Por nivel de significación $\alpha$, la región crítica para el rechazo de la hipótesis de aleatoriedad es
$$
Q>\chi _{1-\alpha ,h}^{2}
$$

donde $\chi _{1-\alpha ,h}^{2}$ es la $\alpha$-cuantil de la distribución chi-cuadrado con $m$ grados de libertad.

La prueba de Ljung-Box se utiliza comúnmente en autorregresivo integrado de media móvil de modelado (ARIMA). Tenga en cuenta que se aplica a los residuos de un modelo ARIMA equipada, no en la serie original, y en tales aplicaciones, la hipótesis de hecho objeto del ensayo es que los residuos del modelo ARIMA no tienen autocorrelación. Al probar los residuales de un modelo ARIMA estimado, los grados de libertad deben ser ajustados para reflejar la estimación de parámetros. Por ejemplo, para un modelo $ARIMA (p,0,q)$, los grados de libertad se debe establecer en $h-p-q$.

```{r}
Box.test(residuos,lag=20,type="Ljung")
```

$Ho$: Ruido Blanco ¿Es ruido blanco?


Probemos un segundo modelo

```{r}
modelo2 <- arima(ts.precio, order=c(2,0,0))
print(modelo2)
```

Comparemos los resultados:

```{r}
ar2.precio <- ar(ts.precio,FALSE,2)
ar2.precio$aic
modelo2$aic
modelo1$aic
```

Se escoge el modelo de menor AIC.

# Proceso de Medias Móviles (MA)

Recordemos el polinomio de rezagos:

$$
B_p{(L)} = \beta_0+\beta_1L+\beta_2L^2+\cdots++\beta_pL^p
$$

combinados con una serie de tiempo:

$$
B_p{(L)}(Y_t) = (\beta_0+\beta_1L+\beta_2L^2+\cdots++\beta_pL^p)(Y_t)
$$

$$
B_p{(L)}(Y_t) =\sum_{j=0}^{p}\beta_jL^jY_t
$$
$$
B_p{(L)}(Y_t) =\sum_{j=0}^{p}\beta_jL^jY_t
$$

**Definición**

Se dice que una serie $Y_t$ sigue un proceso $MA(q)$, $q =1, 2,\ldots$ de media móvil de orden $q$, si se cumple que

$$
Y_t = \epsilon_t+\theta_1\epsilon_{t-1}+\cdots+\theta_q\epsilon_{t-q}
$$
para constantes $\theta_1,\ldots,\theta_p$ y $\epsilon_t\sim N(0,\sigma^2)$. La expresión con el operador $L$ es, si se define el polinomio.

$$
\theta_p(L) = 1+\theta_1L+\cdots+\theta_qL^q
$$

entonces la ecuación queda $Y_t = \theta_q(L)(\epsilon_t)$

**Propiedades**

1. $E(Y_t)=0$
2. $Var(Y_t)= (1+\theta_1^2+\cdots+\theta_q^2)\sigma^2$

luego $Var(Y_t) > Var(\epsilon_t)$, en general.
3. $Cov(Y_t,Y_{t+k}) = R(k)$, donde

$$
R(K) = \sigma^2\sum_{j=0}^{q-k}\theta_j\theta_{j+k}\label{cov}
$$

donde $\theta_0=1$ y $k<q+1$. $R(K)=0$ si $k\geq q+1$.

4. Un $MA(q)$ siempre es un proceso estacionario con ACF, $p(k)=\frac{R(k)}{R(0)}$

La ecuación $\eqref{cov}$ se puede interpretar como una indicación de que un $MA(q)$ es un proceso d´ebilmente correlacionado, ya que su autocovarianza es cero a partir de un valor. Por esta razón se puede ver los procesos $MA(q)$ como alternativas al Ruido Blanco completamente incorrelacionado.

**Ejemplo**

Sea $Y_t\sim MA(2)$ dado por:

$$
y_t = \epsilon_t+\theta_1\epsilon_{t-1}+\theta_2\epsilon_{t-2}
$$
donde $\epsilon_{t} \sim N(0,9)$, con $\theta_1=-0.4$,$\theta_2=0.4$.

De acuerdo con $\eqref{cov}$, si la fac muestral de una serie Yt termina abruptamente puede tratarse de un $MA(q)$.




Simulemos un modelo:

```{r}
simulcion.ma <- arima.sim(200,model=list(ma=c(0.8)))
plot(simulcion.ma)
acf(simulcion.ma)
pacf(simulcion.ma)
```


-   Las $p$ primeras autocorrelaciones van a ser diferentes de cero
-   La autocorrelación parcial decae exponencialmente


Veamos otro ejemplo


```{r}
simulcion.ma1 <- arima.sim(200, model =list(ma=c(2.1,-0.9,4.7)))
plot(simulcion.ma1)
acf(simulcion.ma1)
pacf(simulcion.ma1)
```


# Proceso ARMA

**Definición**

Un proceso $Yt \sim ARMA(p, q)$ se define mediante

$$
\phi_p(L)(Y_t)=\theta_q(L)\epsilon_t
$$
donde $\epsilon_t\sim RB(0,\sigma^2)$ y $\phi_p(z)=1-\sum_{j=1}^{p}\phi_jz^j$, $\theta_q(z)=1+\sum_{j=1}^q\theta_jz^j$ son los polinomios autoregresivo y de media móvil respectivamente.

se asume que las raíces de las ecuaciones $\phi_p(z) = 0$ y $\theta_q(z) = 0$ están fuera del círculo unitario. Además se asume que estos polinomios no tienen raíces en común. Si se cumplen estas condiciones el proceso $Yt \sim ARMA(p, q)$ es estacionario e identificable.

Simulemos el proceso:

```{r}
simulcion.ma2 <- arima.sim(1000, model=list(order=c(1,0,1),ar=c(-0.1),ma=c(0.1)))
plot(simulcion.ma2)
acf(simulcion.ma2)
pacf(simulcion.ma2)

```


# Buscando el *mejor* modelo


### Ejemplo 1

Datos: Serie de tiempo (1952-1988) del ingreso nacional real en China por sector (año base: 1952)

```{r,warning=FALSE,message=FALSE}
library(AER)
data(ChinaIncome)
str(ChinaIncome)
transporte <- ChinaIncome[,"transport"]
ts.plot(transporte)
```


Parece no ser estacionario. Hacemos una transformación para tratar de confirmar la estacionariedad.

```{r}
ltrans <- log(transporte)
ts.plot(ltrans)
```

Notamos que persiste el porblema, sigue sin ser estacionario. Probemos con la diferencia:


```{r}
dltrans <- diff(ltrans)
ts.plot(dltrans)
abline(h=0)
```

Asumamos estacionariedad  (después haremos una prueba específica para verificar estacionariedad) y busquemos el mejor modelo.

Usaremos el `acf` y el `pacf` para evaluar si es MA o AR.

```{r}
acf(dltrans)
pacf(dltrans)
```

Ajustando según la gráfica, tendríamos un proceso $MA(2)$

¿Qué recomienda `R`?

```{r}
ar(dltrans)$aic
```

Según esta recomendación, estamos ante un proceso $AR(3)$.

```{r}
modelo1 <- arima(dltrans,order = c(3,0,0))
modelo1$aic
```

Ajustemos el $MA(2)$ y comparemos:

```{r}
modelo2 <- arima(dltrans,order = c(0,0,2))
modelo2$aic
```

Recuerda: Un menor AIC es mejor. ¿Con qué modelo te quedas?

Ajustemos un $MA(3)$:

```{r}
modelo3 <- arima(dltrans,order = c(0,0,3))
print(modelo3)
modelo3$aic
```

Este modelo es mejor que el $MA(2)$, pero peor que $AR(3)$.


Probemos algunas combinaciones


```{r}
# Ajustando un ARMA(1,1)
modelo4 <- arima(dltrans,order = c(1,0,1))
modelo4$aic
# Ajustando un ARMA(2,1)
modelo5 <- arima(dltrans,order = c(2,0,1))
modelo5$aic
# Ajustando un ARMA(1,2)
modelo6 <- arima(dltrans,order = c(1,0,2))
modelo6$aic
```

Nos quedamos con el $AR(3)$. Revisemos los residuos:

```{r}
AR3Resid <- (modelo1$resid)
ts.plot(AR3Resid)
acf(AR3Resid)
pacf(AR3Resid)
```
No hay autocorrelación, No hay autocorrelación parcial.


Veamos si se trata de un ruido blanco
```{r}
Box.test(AR3Resid)
```

¿Es ruido blanco?


Realicemos una proyección a 5 años


```{r}
pred5 <- predict(modelo1, n.ahead=5,se=T)
pred5se <- pred5$se
```


intervalos de confianza:
```{r}
ic = pred5$pred + cbind(-2*pred5$se,2*pred5$se)
ts.plot(dltrans,pred5$pred,ic,
col=c("black","blue","red","red"))
```

Los intervalos son grandes, podría ser por la cantidad de datos

### Ejemplo 2

Datos: Índice de desempleo trimestal en Canada desde el 62

```{r}
uu <- "https://raw.githubusercontent.com/vmoprojs/DataLectures/master/CAEMP.DAT"
datos <- read.csv(url(uu),sep=",",header=T)
emp.ts <- ts(datos,st=1962,fr=4)
plot(emp.ts)
```


Veamos sus autocorrelaciones y AIC:

```{r}
acf(emp.ts)
pacf(emp.ts)
ar(emp.ts)$aic
```

Decae linealmente el ACF, esto es señal de que no es un proceso AR

Comparemos modelos:

```{r}
mode1 <- arima(emp.ts,order=c(2,0,0))
mode2 <- arima(emp.ts,order=c(0,0,4))
Box.test(mode1$resid,t="Ljung",lag=20)
Box.test(mode2$resid,t="Ljung",lag=20)
```

mode1 es ruido, pero mode2 no lo es. Analicemos los resíduos:

```{r}
ts.plot(mode1$resid)
ts.plot(mode2$resid)
tsdiag(mode1)
```


Probemos un modelo ARMA

```{r}
arma.21 <- arima(emp.ts,order=c(2,0,1))
arma.21$aic
arma.21

tsdiag(arma.21)

arma.21$coef
arma.21$var.coef
```



```{r}
polyroot(c(1,-1.57,0.59)) # Estacionario (Las raíces son |x|>1)
polyroot(c(1,-0.16)) # Invertible
# Si se cumplen ambas, el proceso ARMA es estacionario. 
```



**Condición de invertibilidad del Proceso $MA(q)$**

Dado un proceso $MA(q)$, $Y_t = \theta q(L)(\epsilon_t)$ donde $\theta_q(L) = 1+\theta_1L+\theta_2L^2+\cdots+\theta_qL^q$, entonces considerando el polinomio en $z\in \mathbb{C}, \theta_q(z) = 1+\theta_1z+\cdots+\theta_qz^q$ y sus $q$ raíces $(z_1, z_2,\ldots,z_q)\in \mathbb{C}$, es decir, valores $z \in \mathbb{C}$ tales que $\theta_q(z) = 0$, se dice que el proceso $Y_t$ es invertible si se cumple

$$
|z_j|>1, \quad \forall j = 1,\ldots,q \label{eq1}
$$
o también, si $\theta_q(z) \neq 0,\forall z, |z|\leq  1$. Note que $\eqref{eq1}$ es equivalente a

$$
\frac{1}{z_j}<1, \quad \forall j = 1,\ldots,q
$$

es decir, los inversos de las raíces deben caer dentro del círculo unitario complejo.


## Test de Dickey Fuller

> La Prueba de Dickey-Fuller busca determinar la existencia o no de raíces unitarias en una serie de tiempo. La hipótesis nula de esta prueba es que existe una raíz unitaria en la serie.

```{r,warning=FALSE,message=FALSE}
library(tseries)
adf.test(emp.ts)
adf.test(diff(emp.ts))
```

### Ejemplo 3

Datos: 14 series macroeconómicas: 

  -   indice de precios del consumidor (`cpi`), 
  -   producción industrial (`ip`),
  -   PNB Nominal (`gnp.nom`), 
  -   Velocidad (`vel`), 
  -   Empleo (`emp`), 
  -   Tasa de interés (`int.rate`), 
  -   Sueldos nominales (`nom.wages`), 
  -   Deflactor del PIB (`gnp.def`), 
  -   Stock de dinero (`money.stock`), 
  -   PNB real (`gnp.real`), 
  -   Precios de stock (`stock.prices`), 
  -   PNB per cápita (`gnp.capita`), 
  -   Salario real (`real.wages`), y 
  -   Desempleo (`unemp`). 

Tienen diferentes longitudes pero todas terminan en 1988. Trabajaremos con `cpi`

```{r}
data(NelPlo)
plot(cpi)
```
La serie parece no ser estacionaria ni lineal.

Veamos las raíces unitarias:

```{r}
adf.test(cpi)
```

¿Es estacionaria?

Probemos con las diferencias

```{r,warning=FALSE}
dife <- diff(cpi)
plot(dife)
adf.test(dife)
```

La serie en diferencias si es estacionaria. Veamos qué modelo sugiere R:

```{r}
ar(dife)
```

Hasta mi última revisión, no existe una función `ma` como `ar`, pero:

```{r}
#### Busquemos el mejor MA #####
N=10
AICMA=matrix(0,ncol=1,nrow=N)
for (i in 1:N){
AICMA[i] = arima(diff(cpi),order=c(0,0,i))$aic
}
which.min(AICMA)
AICMA
```

Se sugiere un $MA(3)$.

Evaluemos el modelo MA con una diferencia:

```{r}
mode1 <- arima(cpi,order=c(0,1,3))
mode1
tsdiag(mode1)
```

# Cointegración:

Datos: tasas de cambio mensuales de Estados Unidos, Inglaterra y Nueva Zelanda desde 2004.

```{r}
uu <- "https://raw.githubusercontent.com/vmoprojs/DataLectures/master/us_rates.txt"
datos <- read.csv(url(uu),sep="",header=T)
```


```{r}
# Tasas de cambio, datos mensuales
uk.ts <- ts(datos$UK,st=2004,fr=12)
eu.ts <- ts(datos$EU,st=2004,fr=12)
```


Revisemos si las series son estacionarias:

```{r}
# Tets de Phillips Perron
pp.test(uk.ts)
pp.test(eu.ts)
```

Tienen raíces unitarias

Objetivo: Se piensa que la libra esterlina y el euro tienen alguna relación

```{r}
#Test de Phillips Ollearis
po.test(cbind(uk.ts,eu.ts))
```

La $Ho$: NO COINTEGRADAS.

Si son cointegradas, es decir que hay una tendencia a largo plazo.

Veamos la relación:

```{r}
reg <- lm(uk.ts~eu.ts)
summary(reg)
```

Analicemos los resíduos:

```{r}
residuos = resid(reg)
plot(resid(reg),t="l")
```

Presentan una estructura, debemos modelizarlos.

```{r}
arma11 <- arima(residuos,order=c(1,0,1))
arma11
tsdiag(arma11)
```

Encontramos un modelo en los errores que si es estacionario, la relación a largo plazo entonces es el coeficiente de la regresión: $0.58$.

# Referencias